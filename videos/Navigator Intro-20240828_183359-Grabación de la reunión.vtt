WEBVTT

1
00:00:00.000 --> 00:00:05.000
 On top of Python, I was saying,

2
00:00:05.000 --> 00:00:08.640
 we are using a lot of stuff.

3
00:00:08.640 --> 00:00:12.840
 We have three main service, I think.

4
00:00:12.840 --> 00:00:15.580
 The service for the core resource,

5
00:00:15.580 --> 00:00:28.740
 the APIs, and the flow task that is the application for and for Tasa, how you say Jesus,

6
00:00:28.740 --> 00:00:49.800
 the graphical diagram flow task is like. is a DAX execution giant. It's like our high flow. That was the thing is we start developing,

7
00:00:53.620 --> 00:00:57.840
 development, the development of flow task

8
00:00:57.840 --> 00:01:01.760
 is because we have five years now

9
00:01:01.760 --> 00:01:03.880
 where when we started high flow,

10
00:01:03.880 --> 00:01:05.000
 it was...

11
00:01:05.000 --> 00:01:10.000
 No, seven years. We have seven years working on this.

12
00:01:10.000 --> 00:01:15.000
 Yeah. Guys, we have to separate...

13
00:01:15.000 --> 00:01:35.720
 We had separate ideas about dealing with tasks. For example, every task is a JSON or a JML script. We are

14
00:01:35.720 --> 00:01:49.040
 not writing Python code to running tasks. Every task is component-based, is one component, is a sequence of components,

15
00:01:49.680 --> 00:02:10.560
 is a sequence, we can create cycles or loops, etc. And it is growing very well in the company.

16
00:02:10.560 --> 00:02:19.200
 We are providing the 90% of the data integration in the company right now.

17
00:02:19.200 --> 00:02:27.000
 But mostly we are creating integration for FTP servers, etc.

18
00:02:27.000 --> 00:02:45.500
 Okay, let me share my screen one second to show us some examples of tasks created in Flotus. Give me a second, for sure.

19
00:02:45.500 --> 00:02:46.320
 It's free.

20
00:02:48.080 --> 00:02:53.080
 Let me start my Visual Studio.

21
00:02:57.160 --> 00:02:58.160
 This Flotus.

22
00:03:00.840 --> 00:03:02.820
 This is important because Flotus,

23
00:03:02.820 --> 00:03:07.060
 and that integrator, is our main tool for cleaning data,

24
00:03:07.060 --> 00:03:10.080
 cleaning data management, data ingest.

25
00:03:10.080 --> 00:03:13.560
 So it's an all-in-one tool we have

26
00:03:13.560 --> 00:03:17.100
 created for all the needs that have the company.

27
00:03:17.100 --> 00:03:18.900
 So here we have,

28
00:03:18.900 --> 00:03:21.700
 the only thing we don't have here is the analysis,

29
00:03:21.700 --> 00:03:26.080
 and that is something we can achieve because we can create

30
00:03:26.080 --> 00:03:32.640
 components. But the main goal in the start of beginning of the building of this was to

31
00:03:33.440 --> 00:03:48.000
 abstraction for the developer and the create process for lot data. For example, this is an example of a task.

32
00:03:48.000 --> 00:03:54.920
 This is in JML version of syntax.

33
00:03:54.920 --> 00:03:58.120
 If you see there are several steps,

34
00:03:58.120 --> 00:03:59.600
 every step is a component.

35
00:03:59.600 --> 00:04:05.000
 This component is for downloading a file from an FTP site.

36
00:04:06.840 --> 00:04:11.780
 If this is checking if the files already exist in the site,

37
00:04:11.780 --> 00:04:16.779
 this could open this CSV into a Pandas data frame.

38
00:04:17.140 --> 00:04:22.140
 Everything inside of Flautas is, the data is,

39
00:04:24.860 --> 00:04:25.000
 we are using Pandas for the transformation.

40
00:04:27.680 --> 00:04:32.480
 For example, this company called map is making a mapping

41
00:04:32.480 --> 00:04:36.760
 of, we have Apache ABRA format,

42
00:04:36.760 --> 00:04:39.080
 and okay, I take in the data frame

43
00:04:39.080 --> 00:04:44.080
 and apply this mapping to match the coordinates to the map.

44
00:04:48.920 --> 00:04:58.440
 This making that transformation automatically or we can use also a component called transform rows to transformation of rows by taking the

45
00:04:58.440 --> 00:05:10.880
 for example the kiosk status and convert to boolean, pass set on some rules, there are several functions to be applied

46
00:05:10.880 --> 00:05:20.360
 to transformation and also those functions are using Pandas transformation.

47
00:05:20.360 --> 00:05:26.960
 Everything is a Pandas data frame right now in Navigator. This is another component.

48
00:05:26.960 --> 00:05:31.640
 It's making a join between the data frame and data

49
00:05:31.640 --> 00:05:32.960
 collected from the database.

50
00:05:36.560 --> 00:05:38.480
 This is a component for filtering.

51
00:05:38.480 --> 00:05:44.960
 And this is the component is saving the data in a table.

52
00:05:44.960 --> 00:05:45.000
 This is the name in a table.

53
00:05:47.680 --> 00:05:48.880
 This is the name of the table and this is the schema.

54
00:05:53.880 --> 00:05:57.160
 This company is using Postgres for the flavor

55
00:06:00.000 --> 00:06:02.000
 or the database and is connecting directly to the Postgres and saving the data into it.

56
00:06:02.000 --> 00:06:11.440
 This is JML, but also we have JSON. We are supporting also JSON format.

57
00:06:11.440 --> 00:06:18.240
 It's the same. This is a query. This is a query to a database. The query is converted into a

58
00:06:18.240 --> 00:06:25.460
 Pandas DataFrame. This is filtering, removing columns we don't need it,

59
00:06:25.460 --> 00:06:28.820
 making some transformation of fields,

60
00:06:28.820 --> 00:06:35.420
 making another query, making a join between the bot datasets,

61
00:06:35.420 --> 00:06:38.740
 the query and the transformation,

62
00:06:38.740 --> 00:06:41.880
 making another transformation,

63
00:06:41.880 --> 00:06:45.360
 and then saving into a Postgres

64
00:06:45.360 --> 00:06:48.560
 database table.

65
00:06:48.560 --> 00:06:53.040
 This is the primary key of the table.

66
00:06:53.040 --> 00:06:57.880
 This company also is a oops-er company.

67
00:06:57.880 --> 00:07:00.920
 He's trying to insert the data.

68
00:07:00.920 --> 00:07:04.560
 If the data is not inserted, he's

69
00:07:04.560 --> 00:07:07.860
 trying to make an update.

70
00:07:09.020 --> 00:07:11.600
 Where does the raw data that you're,

71
00:07:11.600 --> 00:07:13.660
 these are like ETL tasks that you're

72
00:07:13.660 --> 00:07:16.420
 pushing out into Navigator, correct?

73
00:07:16.420 --> 00:07:18.900
 Where is the raw data stored?

74
00:07:18.900 --> 00:07:23.860
 Where was that CSV file in the first kiosk YAML?

75
00:07:23.860 --> 00:07:31.560
 Yeah. For example, when we don't know the data,

76
00:07:31.560 --> 00:07:37.400
 we are saving into a network file system now, Javier?

77
00:07:37.400 --> 00:07:50.600
 This directory, we are saving the data passed on the date into a network file system, no, Javier?

78
00:07:50.600 --> 00:07:56.040
 Yeah, exactly. We save all the real files we get for the clients,

79
00:07:56.040 --> 00:08:00.540
 and even for processing or maybe something.

80
00:08:00.540 --> 00:08:03.800
 That is a task we made from the beginning.

81
00:08:03.800 --> 00:08:07.600
 We have all the historical files from all the clients.

82
00:08:07.600 --> 00:08:08.720
 Start.

83
00:08:08.720 --> 00:08:11.680
 Then where do these get pushed to?

84
00:08:11.680 --> 00:08:16.320
 Like how would someone like me access the output of this ETL flow?

85
00:08:19.320 --> 00:08:22.200
 The kind of the ETL, for example,

86
00:08:22.200 --> 00:08:26.220
 the output of this ETL for example, the output of this ETL is,

87
00:08:29.720 --> 00:08:32.820
 if we don't provide the credential,

88
00:08:32.820 --> 00:08:36.280
 the component is connected with the default database,

89
00:08:36.280 --> 00:08:50.580
 and the default database is our mind Postgres database. I think. Okay, so in order for me to start pulling data from

90
00:08:50.580 --> 00:08:58.820
 Navigator, let's say I wanted to pull sales data that you guys

91
00:08:58.820 --> 00:09:04.820
 are pushing out through one of these either JSON or YAML scripts.

92
00:09:04.820 --> 00:09:08.680
 What's the best way for me to get started on doing that?

93
00:09:08.760 --> 00:09:11.600
 From Jupyter maybe.

94
00:09:11.600 --> 00:09:17.440
 Using our Jupyter server to getting data using,

95
00:09:17.440 --> 00:09:21.540
 first, our database connectors.

96
00:09:21.540 --> 00:09:23.280
 We are using Python.

97
00:09:23.280 --> 00:09:26.640
 We have a library.

98
00:09:26.640 --> 00:09:29.520
 Let me show you.

99
00:09:29.520 --> 00:09:31.840
 Let me start my Jupyter server.

100
00:09:35.680 --> 00:09:41.440
 We have, for example, we have several ways to get in data.

101
00:09:41.440 --> 00:09:45.720
 One is using database connector directly.

102
00:09:45.720 --> 00:09:51.320
 We can provide to you the environments variables

103
00:09:51.320 --> 00:09:56.000
 to getting data, to getting the credential for the Postgres,

104
00:09:56.000 --> 00:10:01.120
 InfluxDB, Rec-INDB, SQL Server, et cetera, et cetera.

105
00:10:01.120 --> 00:10:05.000
 That is the first way, using the Direct Database Connector.

106
00:10:05.000 --> 00:10:09.000
 We have also a library called asyndb.

107
00:10:09.000 --> 00:10:15.000
 Asyndb is an abstraction for using the same syntax to get data from different sources.

108
00:10:15.000 --> 00:10:26.040
 We have BigQuery, Influx, Lotta, Postgres, Minestrel, SQL Server, etc.

109
00:10:26.040 --> 00:10:29.560
 The third way is using query source.

110
00:10:29.560 --> 00:10:35.400
 Query source is a library for getting

111
00:10:35.400 --> 00:10:40.640
 we have an infrastructure to saving queries.

112
00:10:40.640 --> 00:10:44.240
 The client, he needs a query.

113
00:10:44.240 --> 00:10:49.320
 He create a query, okay, this query is, we provide to

114
00:10:49.320 --> 00:11:13.960
 the query with a name, okay. Sorry. And then we have those named queries, we have a library to get in programmatically that

115
00:11:13.960 --> 00:11:14.960
 data.

116
00:11:14.960 --> 00:11:15.960
 Okay.

117
00:11:15.960 --> 00:11:31.360
 Mostly that is right now the mine consumer because every saved query can be accessed using a RESTful API.

118
00:11:31.360 --> 00:11:52.560
 And right now the company is using Power BI to consuming those RESTful APIs. Okay. And also the third way is using directly pandas etc on the tools we are already

119
00:11:52.560 --> 00:12:06.720
 integrating Navigator and getting data directly from the from the from the network five system directory.

120
00:12:10.580 --> 00:12:15.580
 That is the ways we have to provide data for you. Okay, let me show you my Jupyter.

121
00:12:15.620 --> 00:12:16.960
 Give me a second.

122
00:12:19.760 --> 00:12:20.600
 One moment.

123
00:12:31.240 --> 00:12:35.320
 Okay. This is my local, but we have already the Jupyter server

124
00:12:35.320 --> 00:13:05.000
 serving in production or even in that. For example, let me show you for database connection. I think this is good.

125
00:13:05.000 --> 00:13:25.500
 Okay, this is the first way we used to, sorry, it's in Spanish because I made for my people, We are getting a CDB connection for Postgres

126
00:13:26.820 --> 00:13:28.820
 making a connection

127
00:13:29.220 --> 00:13:32.480
 Starting a connection using a Singapore syntax

128
00:13:33.060 --> 00:13:34.280
 getting

129
00:13:34.280 --> 00:13:36.280
 Saying I need it in pandas

130
00:13:37.300 --> 00:13:43.260
 Getting the data provided by the query. That's it. That is the only

131
00:13:46.240 --> 00:13:47.360
 So provided by the query that's it that that is the only uh so

132
00:13:59.280 --> 00:14:05.960
 i'm sorry i forgot that i removed a ip one okay um free and there is this is my data frame I remove the IP point.

133
00:14:09.720 --> 00:14:12.720
 There it is. This is my DataFrame.

134
00:14:16.720 --> 00:14:18.780
 Am I able to just run these commands and will this work on my machine or do I need to set up

135
00:14:18.780 --> 00:14:20.920
 some environment in order to get this to work?

136
00:14:20.920 --> 00:14:25.920
 Yes. Because we have VPN.

137
00:14:27.920 --> 00:14:34.480
 Currently, we can provide to you for an environment, an environment for the project

138
00:14:34.480 --> 00:14:51.360
 to run it, to connect it directly with the sources in depth or staging or production. Let me show you how we can use that. Let me stop here.

139
00:14:52.320 --> 00:15:13.600
 Let me start it again.

140
00:15:13.600 --> 00:15:21.560
 Let me check the...

141
00:15:21.560 --> 00:15:31.680
 One moment. Let me check the iPads.

142
00:15:31.680 --> 00:15:45.280
 Let me check mine. Yes.

143
00:15:45.280 --> 00:15:47.560
 When I start my project in local,

144
00:15:47.560 --> 00:15:51.600
 I can point to the production environment.

145
00:15:51.600 --> 00:15:57.440
 The production environment is a single environment file

146
00:15:57.440 --> 00:16:00.200
 with the credential of all databases.

147
00:16:00.200 --> 00:16:04.760
 When I start pointing to production,

148
00:16:04.760 --> 00:16:10.720
 I can right now start using queries directly from production.

149
00:16:21.200 --> 00:16:31.000
 Yes, because I stopped at the server.

150
00:16:36.000 --> 00:16:45.800
 One point important here is that you can have the best of the world. I forgot to start Jupyter in my. Yeah, this is what you can. Really, you have.

151
00:16:45.800 --> 00:16:49.400
 You can have the base of two words you can

152
00:16:49.480 --> 00:16:53.640
 have in the in your own bare machine installed the

153
00:16:53.640 --> 00:16:57.720
 project and connect to through the VPN to the service

154
00:16:57.800 --> 00:17:01.840
 and the data endpoints or use a software we have

155
00:17:01.920 --> 00:17:05.240
 to do this stuff on the through the VPN,

156
00:17:05.240 --> 00:17:08.920
 but we did with the browser or other tools.

157
00:17:08.920 --> 00:17:14.400
 It's like, go ahead.

158
00:17:14.400 --> 00:17:21.400
 You can connect, you're ruining the project in your local,

159
00:17:21.400 --> 00:17:24.300
 but using the VPNs to

160
00:17:24.300 --> 00:17:28.119
 connect into that to prop up to a staging directly.

161
00:17:28.119 --> 00:17:31.680
 You don't need to. You don't need to use local

162
00:17:31.680 --> 00:17:33.240
 database or something.

163
00:17:33.240 --> 00:17:38.520
 OK. So how do I get this set up on my computer?

164
00:17:38.520 --> 00:17:42.160
 Uh. He held you with that.

165
00:17:42.160 --> 00:17:45.760
 Yeah, it is, but It's not so difficult.

166
00:17:45.760 --> 00:17:47.520
 It's a gift repo.

167
00:17:47.520 --> 00:17:51.240
 You clone the repo and running and making start in your local.

168
00:17:51.240 --> 00:17:51.680
 Yeah.

169
00:17:51.680 --> 00:17:54.880
 Okay. That's what I need to get

170
00:17:54.880 --> 00:17:59.520
 the VPN and environment set up on my computer?

171
00:17:59.520 --> 00:18:02.040
 Yeah. That is something that's new.

172
00:18:02.040 --> 00:18:03.440
 I can help you with that.

173
00:18:03.440 --> 00:18:12.840
 Okay. Let me find your e-mail to start generating all the VPN.

174
00:18:19.420 --> 00:19:27.000
 For example, we have a library called a QS and QS this library. That's it. This is in production. I don't have that data in my local. This is a data temporary... ah no no no no

175
00:19:27.000 --> 00:19:34.000
 Javier, we don't have access to Odoo from VPN

176
00:19:34.000 --> 00:19:38.000
 find it to local resolution

177
00:19:38.000 --> 00:19:45.000
 we don't have, I don't have access to the Odoom from the VPN.

178
00:19:45.360 --> 00:19:49.040
 I checked it yesterday.

179
00:19:49.040 --> 00:19:50.360
 Oh, got it, got it.

180
00:19:50.360 --> 00:19:51.360
 Let me check it.

181
00:19:51.360 --> 00:19:53.120
 Let me prove with another.

182
00:19:54.480 --> 00:19:55.780
 Let's look.

183
00:19:55.780 --> 00:19:58.760
 This is the list of Pokemon stores.

184
00:20:02.280 --> 00:20:03.760
 And there is, I have the data.

185
00:20:03.760 --> 00:20:07.860
 This is, here is the result.

186
00:20:07.860 --> 00:20:12.140
 You see, all the data provided.

187
00:20:12.140 --> 00:20:12.980
 Okay?

188
00:20:13.980 --> 00:20:15.740
 It's so simple.

189
00:20:15.740 --> 00:20:19.920
 You only need to know which is the name and query

190
00:20:19.920 --> 00:20:24.920
 you need to run getting from QS the query

191
00:20:25.000 --> 00:20:26.760
 and that's it.

192
00:20:28.740 --> 00:20:30.240
 In the result you have the,

193
00:20:34.920 --> 00:21:00.000
 right now it's a list of dictionaries but you can use pandas for conversion into a data frame.

194
00:21:04.000 --> 00:21:08.600
 That's it. Okay, and you can start, you know, using Pandas libraries.

195
00:21:08.600 --> 00:21:19.060
 So to use the query source package,

196
00:21:19.060 --> 00:21:22.160
 that's a package that you guys built?

197
00:21:22.160 --> 00:21:26.000
 Okay. That's what I need the environment

198
00:21:26.000 --> 00:21:27.900
 and the VPN for, right?

199
00:21:27.900 --> 00:21:29.140
 To use that package?

200
00:21:29.140 --> 00:21:30.080
 Okay, cool.

201
00:21:30.080 --> 00:21:32.020
 I just wanna make sure I understand.

202
00:21:32.020 --> 00:21:33.900
 Sorry, I'm just taking some notes here.

203
00:21:33.900 --> 00:21:34.880
 Give me one sec.

204
00:21:36.940 --> 00:21:38.860
 And is there somewhere where,

205
00:21:38.860 --> 00:21:42.960
 so once I get this running on my machine,

206
00:21:42.960 --> 00:21:46.200
 how will I find the names of all the different queries?

207
00:21:46.200 --> 00:21:49.180
 How would I know what the Pokemon underscore stores one is?

208
00:21:49.180 --> 00:21:52.180
 Is there a list or directory of that somewhere?

209
00:21:53.220 --> 00:22:01.080
 No, but those names are saved into a Postgres table.

210
00:22:01.080 --> 00:22:09.760
 You can query directly to public dot query table and there are I think I think I

211
00:22:09.760 --> 00:22:17.280
 think Robin refers to if we have documented all this loop and something like that what belongs

212
00:22:17.280 --> 00:22:35.240
 and where is stores and worries I think we don't that is something more operational, but we don't have that. Even we create the name, this look is a program underscore process or model.

213
00:22:35.240 --> 00:22:36.240
 Right.

214
00:22:36.240 --> 00:22:43.600
 For example, Pokemon underscore stores are the stores of the Pokemon project.

215
00:22:43.600 --> 00:22:46.800
 And this is something is replicated on all the tenants.

216
00:22:46.800 --> 00:22:50.480
 You want to see Exxon store, you know that.

217
00:22:50.480 --> 00:22:52.320
 Exxon under the score.

218
00:22:52.320 --> 00:22:55.240
 You can see exactly like, instead of Pokemon,

219
00:22:55.240 --> 00:23:00.240
 epsilon is equal or Walmart or...

220
00:23:01.720 --> 00:23:04.920
 There are several conventions in Navigator.

221
00:23:04.920 --> 00:23:07.000
 There are several conventions. Navigator. There are several conventions.

222
00:23:07.000 --> 00:23:10.640
 I'll just have to become familiar with it myself.

223
00:23:10.640 --> 00:23:12.900
 Yeah. For the task,

224
00:23:12.900 --> 00:23:19.880
 it's the same. For the task as a program,

225
00:23:19.880 --> 00:23:21.240
 and the name of the task,

226
00:23:21.240 --> 00:23:30.000
 is the convention is the same.

227
00:23:35.280 --> 00:23:40.400
 For example, let me show you how to run a task. You see the task here in flow task.

228
00:23:40.400 --> 00:24:25.500
 Oh, give me a second. Oh . Is Jesus frozen?

229
00:24:27.500 --> 00:24:28.700
 I think maybe he lost internet or something

230
00:24:30.780 --> 00:24:31.640
 Yeah, I think I think he's free

231
00:24:36.780 --> 00:24:37.920
 okay, no, I I think I think he he's like that was low because

232
00:24:45.000 --> 00:24:45.120
 He totally freezable. He should have I don't know five or seven projects.7 projects instead of the time

233
00:24:50.120 --> 00:24:50.740
 is well he has a huge machine to do that.

234
00:24:53.680 --> 00:24:54.840
 While he figures while he reboots or whatever, let me ask you a question.

235
00:24:54.840 --> 00:24:58.920
 So what in what context would I wanna use Postgres

236
00:24:58.920 --> 00:25:01.900
 instead of just accessing it through the Python package?

237
00:25:08.760 --> 00:25:08.840
 Well. Is there any reason why I would use Postgres or

238
00:25:12.060 --> 00:25:15.600
 is that just what you guys use to manage the queries? No, well we use the the name a query

239
00:25:15.680 --> 00:25:17.440
 because it's a easy way.

240
00:25:17.440 --> 00:25:21.960
 To provide I am point a real imposter secure endpoint

241
00:25:22.040 --> 00:25:23.160
 to feed data.

242
00:25:23.160 --> 00:25:27.640
 We used to interchange data with the Power BI,

243
00:25:27.640 --> 00:25:29.440
 and even with the clients,

244
00:25:29.440 --> 00:25:31.340
 we provide some tokens,

245
00:25:31.340 --> 00:25:33.360
 authentication, and they can

246
00:25:33.360 --> 00:25:37.240
 feed their system with the data we collect.

247
00:25:37.240 --> 00:25:42.580
 The goal here is you have all the tools we have on your behalf,

248
00:25:42.580 --> 00:25:44.840
 because the goal is you can

249
00:25:44.840 --> 00:25:49.360
 easily get all the information we have on your behalf because the goal is you can easily get all the information we need.

250
00:25:50.720 --> 00:25:59.120
 Because the good thing with the query source is the query source is a name query that could

251
00:25:59.920 --> 00:26:05.400
 track, I don't know, can connect to the Redding DB or BigQuery,

252
00:26:05.400 --> 00:26:10.280
 or Postgres, RDS, Elastic Azure,

253
00:26:10.280 --> 00:26:11.560
 wherever you want it,

254
00:26:11.560 --> 00:26:15.500
 and with a single endpoint, feed all the data.

255
00:26:15.500 --> 00:26:17.420
 You don't have to connect,

256
00:26:17.420 --> 00:26:20.620
 have a client, you create the query and that's it.

257
00:26:20.620 --> 00:26:27.000
 The query makes the magic behind behind so you don't have to

258
00:26:27.080 --> 00:26:31.000
 deal with the connection with the VPN with this and

259
00:26:31.080 --> 00:26:34.640
 that and that is something that if give us to

260
00:26:34.640 --> 00:26:40.240
 the facility too. Make reporting more easily to young mesh

261
00:26:40.320 --> 00:26:42.400
 data from different origin.

262
00:26:44.320 --> 00:26:47.520
 Hello, I think you first too or is my.

263
00:26:53.520 --> 00:27:11.980
 Oh no. I think you're free.

264
00:27:11.980 --> 00:27:13.540
 I don't know guys.

265
00:27:13.540 --> 00:27:55.000
 See? you Hi Javier, sorry, my computer just crashed as well.

266
00:27:55.000 --> 00:28:05.260
 I've had a lot of issues where my computer keeps crashing and I don't know why so I know well if you if you do the work with the with the

267
00:28:09.700 --> 00:28:10.020
 With the data and the same time are connecting to the things and soon

268
00:28:14.440 --> 00:28:15.120
 This something that is going to happen. I don't I love why I have to

269
00:28:20.540 --> 00:28:20.620
 Separate the the laptop that I use for the meetings and zooms and all the stuff

270
00:28:26.200 --> 00:28:32.200
 It's different from the lattice war because it's a pain to working, it start working on something and suddenly crashes the computer because it soon hang up.

271
00:28:32.200 --> 00:28:33.900
 It's a pain.

272
00:28:35.400 --> 00:28:41.400
 Well, I was saying the goal is to have a single point of truth where I can,

273
00:28:41.400 --> 00:28:45.840
 or a single point of tool, a tool that can handle everything I need.

274
00:28:45.840 --> 00:28:50.000
 I don't have to build something for connect me to

275
00:28:50.000 --> 00:28:53.980
 rating or use another application for SQL Server.

276
00:28:53.980 --> 00:28:56.780
 With the query server,

277
00:28:56.780 --> 00:29:01.860
 I can connect to almost 18, 20.

278
00:29:01.860 --> 00:29:05.960
 Also, we can add all the.

279
00:29:05.960 --> 00:29:12.800
 The Java I don't know how how it's called the ORM.

280
00:29:12.800 --> 00:29:17.240
 The Hava has a lead that has support to I don't

281
00:29:17.340 --> 00:29:21.520
 know like 20 database more so we can we can

282
00:29:21.620 --> 00:29:24.180
 we can connect to almost everything here.

283
00:29:24.180 --> 00:29:28.720
 Even with this look this Even with the wonderful stuff with this look,

284
00:29:28.720 --> 00:29:30.880
 we can have, for example,

285
00:29:30.880 --> 00:29:33.640
 data from SQL Server,

286
00:29:34.120 --> 00:29:36.520
 I don't know, from Visits,

287
00:29:36.520 --> 00:29:40.160
 data from the Odoo ERP,

288
00:29:40.160 --> 00:29:41.640
 that is a positive,

289
00:29:41.640 --> 00:29:43.920
 but it's a different server.

290
00:29:43.920 --> 00:29:46.640
 Also, from Navigator, from ETL, that is a process that is a different server. Also, for Navigator, for an ETL,

291
00:29:46.640 --> 00:29:49.000
 that is a process that give those to client and

292
00:29:49.000 --> 00:29:52.160
 we ingest all the data and clean up.

293
00:29:52.160 --> 00:29:56.120
 We can merge, we can make a mashup the data,

294
00:29:56.120 --> 00:29:59.720
 the three-three endpoint or three-three sources of data,

295
00:29:59.720 --> 00:30:02.080
 we can mash up in a single loop.

296
00:30:02.080 --> 00:30:06.000
 So you're only going to call out one is loop

297
00:30:06.000 --> 00:30:09.760
 and it will feed you with data from different sources.

298
00:30:09.760 --> 00:30:13.320
 Without need, it start to spin up,

299
00:30:13.320 --> 00:30:17.120
 net anything technically with,

300
00:30:17.120 --> 00:30:21.360
 I don't know, application or creating Python script, anything.

301
00:30:21.360 --> 00:30:21.960
 It's-

302
00:30:21.960 --> 00:30:28.160
 So the slugs contain queries that have already combined multiple tables

303
00:30:28.160 --> 00:30:36.720
 on the backend basically, right? Exactly. I think that is the most powerful tool we have

304
00:30:36.720 --> 00:30:48.860
 because we are using, I don't know, like five years ago. When we started, we used Django,

305
00:30:48.860 --> 00:30:54.520
 and we create process and we create the ETLs,

306
00:30:54.520 --> 00:31:02.860
 discrete for load data and discrete for API for serve this data.

307
00:31:02.860 --> 00:31:07.640
 We found that there was something that we cannot escalate.

308
00:31:07.640 --> 00:31:10.720
 So that was the stop.

309
00:31:10.720 --> 00:31:15.480
 We stop all the development we were down in Django,

310
00:31:15.480 --> 00:31:17.560
 and start from the scratch with

311
00:31:17.560 --> 00:31:22.760
 these two on top of a sync. Look, Jesus is back.

312
00:31:22.760 --> 00:31:25.960
 Sorry, my computer crashed

313
00:31:25.960 --> 00:31:28.920
 when I tried to move.

314
00:31:28.920 --> 00:31:31.040
 Mine did too, it's okay.

315
00:31:31.040 --> 00:31:32.540
 Robin have the same issue.

316
00:31:34.160 --> 00:31:39.080
 Yeah, we are using Zoom,

317
00:31:39.080 --> 00:31:42.120
 but for some reason we are migrating to Teams

318
00:31:42.120 --> 00:31:45.880
 and Teams is failing too too much my magic.

319
00:31:46.280 --> 00:31:51.560
 Even I use the web version

320
00:31:51.560 --> 00:31:56.400
 instead of the client because the client is very booby.

321
00:31:58.400 --> 00:32:02.020
 For me as an end user,

322
00:32:02.020 --> 00:32:03.880
 it sounds like I'll be able to get everything I

323
00:32:03.880 --> 00:32:08.240
 need just using the query source package, correct?

324
00:32:12.400 --> 00:32:16.800
 I was in mute, sorry. This is something important because,

325
00:32:16.800 --> 00:32:18.940
 for example, I want to know what is

326
00:32:18.940 --> 00:32:22.100
 the context of what you are going to do or what do you need?

327
00:32:22.100 --> 00:32:26.240
 Because the goal is to we can provide you all the stuff you need.

328
00:32:26.240 --> 00:32:30.540
 Even if we can make some improvement of you need something we don't have,

329
00:32:30.540 --> 00:32:31.880
 we can integrate that.

330
00:32:31.880 --> 00:32:37.120
 The goal is where your leg arm,

331
00:32:37.120 --> 00:32:40.320
 where you can all the technical stuff,

332
00:32:40.320 --> 00:32:50.920
 the challenges that you need to tackle, we can help you. Yeah. So I don't have a specific use case I need today.

333
00:32:50.920 --> 00:32:53.480
 I think in general,

334
00:32:53.480 --> 00:33:00.960
 the first things I'll want are store lists and sales data for different clients.

335
00:33:00.960 --> 00:33:04.520
 Those clients primarily being Bose,

336
00:33:04.520 --> 00:33:10.280
 Hisense, and Epson.

337
00:33:10.280 --> 00:33:15.480
 Are those queries, like are those slugs already created, store lists and sales for those three

338
00:33:15.480 --> 00:33:16.480
 clients?

339
00:33:16.480 --> 00:33:22.520
 So maybe what would be helpful for me is we're going to run out of time pretty soon here

340
00:33:22.520 --> 00:33:25.880
 and I have some other pretty urgent stuff to work on this week.

341
00:33:25.880 --> 00:33:28.920
 But maybe early next week, Javier,

342
00:33:28.920 --> 00:33:32.320
 we can set up a call where you help me install the environment,

343
00:33:32.320 --> 00:33:34.760
 and we just run through a few examples of

344
00:33:34.760 --> 00:33:38.440
 me pulling a few slugs using the query source package.

345
00:33:38.440 --> 00:33:41.240
 I'll save some scripts to my computer

346
00:33:41.240 --> 00:33:44.760
 that just give me a template to work off of.

347
00:33:44.760 --> 00:33:47.400
 Then if we need to make any changes to

348
00:33:47.400 --> 00:33:50.340
 those logs or create new ones then we'll go from there.

349
00:33:50.340 --> 00:33:51.880
 Does that make sense to you guys?

350
00:33:51.880 --> 00:33:53.560
 Yeah.

351
00:33:53.920 --> 00:33:57.120
 Even for example, we are creating

352
00:33:57.120 --> 00:34:01.320
 this collection of Jupyter notebooks with some samples.

353
00:34:01.320 --> 00:34:05.000
 For example, we're getting sales data from HiSEM,

354
00:34:06.500 --> 00:34:07.540
 from Epson.

355
00:34:07.540 --> 00:34:12.540
 In Epson, we have in Postgres, we have a paper

356
00:34:12.580 --> 00:34:17.580
 with I think are 100 millions of sales

357
00:34:18.860 --> 00:34:27.920
 of Epson sales data from all United States.

358
00:34:31.040 --> 00:34:38.639
 And also we have a summarization in another database part. But if you need to get in the raw data we got from the client every week, we have a

359
00:34:38.639 --> 00:34:42.719
 table right now in the database. I have a question.

360
00:34:42.719 --> 00:34:45.440
 My suggestion also, we have already

361
00:34:45.440 --> 00:34:50.320
 integrated some EDA, some exploratory data analysis

362
00:34:50.320 --> 00:34:51.440
 inside of Navigator.

363
00:34:51.440 --> 00:34:54.360
 We have gdata profiling.

364
00:34:54.360 --> 00:34:59.760
 gdata profiling is the old pandas profiling.

365
00:34:59.760 --> 00:35:12.960
 Pandas profiling was renamed to gdata profiling. We have Canaries, DigiWorker, SuiteBees, and DataTail.

366
00:35:18.240 --> 00:35:27.100
 You can use some tools of exploratory data analysis already integrated in Navigator,

367
00:35:27.100 --> 00:35:31.100
 like Swiftviz or detail,

368
00:35:33.040 --> 00:35:36.600
 or for example, let me run this example

369
00:35:36.600 --> 00:35:40.360
 for getting data from,

370
00:35:41.420 --> 00:35:42.820
 this is an Excel file.

371
00:35:44.600 --> 00:35:48.760
 If I use PyGWalker, I can import PyGWalker,

372
00:35:48.760 --> 00:35:50.360
 our running PyGWalker.

373
00:35:50.360 --> 00:35:53.880
 PyGWalker creates this beautiful UI

374
00:35:53.880 --> 00:36:02.520
 to getting all the data is which column,

375
00:36:02.520 --> 00:36:05.000
 the distribution of the values,

376
00:36:06.480 --> 00:36:10.000
 if there are unique columns,

377
00:36:10.000 --> 00:36:13.200
 even create some visualization like

378
00:36:14.480 --> 00:36:19.040
 month index axis, sales index axis,

379
00:36:20.600 --> 00:36:26.140
 or can use Dtale,

380
00:36:26.140 --> 00:36:35.860
 I can run in this and Dtale open in another tab.

381
00:36:35.860 --> 00:36:38.240
 We can use the visualization to

382
00:36:38.240 --> 00:36:41.620
 get in relation between the columns.

383
00:36:41.620 --> 00:36:49.520
 There are several tools integrated right now in the Navigator ecosystem you can use for

384
00:36:51.360 --> 00:37:08.520
 starting understanding the data. Sweet Biz is more for that data understanding, you got every column, the values, the most frequent value,

385
00:37:08.520 --> 00:37:18.380
 you got, you know, for example, if you click here in association, you got the correlation

386
00:37:18.380 --> 00:37:27.600
 between columns or something like that. So how is using these analysis tools different from,

387
00:37:27.600 --> 00:37:31.960
 is this the same as just using them

388
00:37:31.960 --> 00:37:36.840
 as on a regular Pandas data frame on my computer?

389
00:37:36.840 --> 00:37:40.160
 Yeah, because the client

390
00:37:40.160 --> 00:37:44.080
 provide us with Excel file or we don't know nothing about the data.

391
00:37:44.080 --> 00:37:45.700
 We are not an Excel file we don't know nothing about the data we are I am not

392
00:37:45.700 --> 00:37:52.880
 an object self file I don't try to understand okay it is inventory or sales

393
00:37:52.880 --> 00:38:05.000
 or something okay we mostly I use party worker or or or the detail is recent.

394
00:38:07.920 --> 00:38:10.060
 It's not, this is new.

395
00:38:10.060 --> 00:38:13.200
 This tool is very new.

396
00:38:14.040 --> 00:38:18.400
 I integrated, we don't use it yet for our production.

397
00:38:19.540 --> 00:38:23.440
 Even, remember, we start the analysis of the data

398
00:38:23.440 --> 00:38:26.840
 and after that we industrialize in the process that we use,

399
00:38:26.840 --> 00:38:29.520
 we query source and aggregator because we create

400
00:38:29.520 --> 00:38:34.360
 the reports and we create a task and all the stuff that is needed.

401
00:38:34.360 --> 00:38:41.460
 Because the goal is the client can reach his data in an easy way.

402
00:38:41.460 --> 00:38:43.160
 So the goal is not,

403
00:38:43.160 --> 00:38:46.120
 I don't give these, for example,

404
00:38:46.120 --> 00:38:49.960
 to Steven, Steven don't have access to something like Jupyter.

405
00:38:49.960 --> 00:38:52.080
 He go to Navigator,

406
00:38:52.080 --> 00:38:58.360
 see a dashboard and see a graphic or maybe see a table with the data,

407
00:38:58.360 --> 00:39:00.680
 or even he can in Navigator,

408
00:39:00.680 --> 00:39:03.520
 in the Navigator visualization tool,

409
00:39:03.520 --> 00:39:09.620
 we can set up a table with data and he can filter all the data,

410
00:39:09.620 --> 00:39:14.980
 export the data and make it like a playground from there.

411
00:39:14.980 --> 00:39:22.960
 But it's a site when he can get all the information he wants.

412
00:39:22.960 --> 00:39:28.260
 That is something I want to ask you because I don't know what is the tool

413
00:39:28.260 --> 00:39:33.660
 that you are going to work with Steven and all the crew

414
00:39:33.660 --> 00:39:38.020
 to show the results or show the reports you are using.

415
00:39:38.020 --> 00:39:39.900
 You're going to use Power BI,

416
00:39:39.900 --> 00:39:41.420
 you're going to use something like that.

417
00:39:41.420 --> 00:39:44.420
 I don't know if we can help you.

418
00:39:44.420 --> 00:39:50.600
 I don't know if we can help you. I don't know if you create some metrics analytics or report,

419
00:39:50.600 --> 00:39:53.560
 we can put on one of the tool we have,

420
00:39:53.560 --> 00:39:56.240
 even Navigator or Superset.

421
00:39:56.240 --> 00:40:00.160
 I don't know. We have a team that can help with that too.

422
00:40:00.160 --> 00:40:02.820
 So the goal is, how I say,

423
00:40:02.820 --> 00:40:05.240
 they show the client we are

424
00:40:06.240 --> 00:40:08.240
 We're in the same

425
00:40:08.240 --> 00:40:14.240
 aspect of tool if we are we need to add a different tool we can add it but the goal is to try to

426
00:40:14.480 --> 00:40:17.960
 integrate with all the ecosystem we have because the important

427
00:40:18.880 --> 00:40:23.380
 Well, there is a there is a thing with the clients is there is a different stuff

428
00:40:23.380 --> 00:40:25.440
 but this is mainly for

429
00:40:25.440 --> 00:40:29.600
 Steven and all the C-level people.

430
00:40:29.600 --> 00:40:30.600
 So it's different.

431
00:40:30.600 --> 00:40:34.200
 It's different than the report that we provide to a client.

432
00:40:34.200 --> 00:40:43.040
 But if we can help you with the representation of all the process that you are going to create,

433
00:40:43.040 --> 00:40:48.280
 I want you to know you can come on us to help you to get that done.

434
00:40:49.180 --> 00:40:53.740
 OK, I I think step one is just getting the access to the data

435
00:40:53.740 --> 00:40:55.500
 itself and then.

436
00:40:55.900 --> 00:41:00.260
 Um, as projects start getting built out, I'll discuss with you

437
00:41:00.260 --> 00:41:04.580
 guys like all the results of those projects are going to end

438
00:41:04.580 --> 00:41:06.660
 up getting reported back.

439
00:41:09.860 --> 00:41:12.500
 Could you guys give me a quick walk-through of what you said,

440
00:41:12.500 --> 00:41:15.060
 Stephen, when Stephen accesses

441
00:41:15.060 --> 00:41:17.100
 Navigator and he's just seeing dashboards,

442
00:41:17.100 --> 00:41:21.980
 what does that look like for him and what tool is that?

443
00:41:21.980 --> 00:41:23.060
 Let me show you.

444
00:41:23.060 --> 00:41:24.900
 Is this something that I can access?

445
00:41:24.900 --> 00:41:25.960
 Yeah. Yeah, sure.

446
00:41:25.960 --> 00:41:28.120
 Right now, for example,

447
00:41:28.120 --> 00:41:31.760
 this is Navigator, you see it?

448
00:41:31.760 --> 00:41:37.040
 Yeah. So I just go to navigator.tirocglobal.com?

449
00:41:37.040 --> 00:41:39.400
 Yeah, and you can log in.

450
00:41:39.400 --> 00:41:45.000
 Navigator.tirocglobal.com, you can log in using SSO with your credential.

451
00:41:45.600 --> 00:41:48.720
 Obviously, we need to provide to you the permission

452
00:41:48.720 --> 00:41:53.720
 to getting access to all programs.

453
00:41:54.260 --> 00:41:58.540
 When you click in a program, you will see some dashboards.

454
00:41:58.540 --> 00:42:03.540
 And also, those dashboards are consuming the same API

455
00:42:04.780 --> 00:42:08.120
 the company used from Power BI.

456
00:42:08.840 --> 00:42:12.600
 For example, this is the regional manager,

457
00:42:12.600 --> 00:42:15.600
 BC by type of error.

458
00:42:15.600 --> 00:42:16.800
 There are-

459
00:42:16.800 --> 00:42:20.160
 These are basically visualizations

460
00:42:20.160 --> 00:42:22.640
 based on query source queries, is that right?

461
00:42:22.640 --> 00:42:23.280
 Exactly.

462
00:42:23.280 --> 00:42:23.520
 Yeah.

463
00:42:23.520 --> 00:42:25.240
 Exactly. All of these right? Exactly. Yeah. Exactly. All the solutions.

464
00:42:25.240 --> 00:42:32.000
 Yeah. All in the ecosystem,

465
00:42:32.000 --> 00:42:35.400
 Navigator, DropRoble, Jupyter,

466
00:42:35.400 --> 00:42:38.240
 it's Power BI integration are

467
00:42:38.240 --> 00:42:42.560
 using in the behind the query sources created.

468
00:42:42.560 --> 00:42:43.680
 Okay.

469
00:42:43.680 --> 00:42:46.780
 For example, DNS is a vc2gold,

470
00:42:46.780 --> 00:42:51.780
 or vc2gold by area, you can filter,

471
00:42:51.820 --> 00:42:54.100
 you can select a single area,

472
00:42:55.580 --> 00:42:59.820
 several markets, and apply, that's it.

473
00:42:59.820 --> 00:43:00.940
 Okay?

474
00:43:00.940 --> 00:43:01.760
 Yeah, no, this makes sense.

475
00:43:01.760 --> 00:43:04.580
 This is equal for every program,

476
00:43:04.580 --> 00:43:06.500
 like Pokemon,

477
00:43:06.500 --> 00:43:11.620
 Pokemon inventory, and it's.

478
00:43:11.780 --> 00:43:16.540
 Could you guys give me access to those three clients,

479
00:43:16.540 --> 00:43:19.540
 Epson, iSense, and Bose?

480
00:43:19.540 --> 00:43:26.480
 Yeah. I'm going to give you a sense. For example, for Sysen,

481
00:43:26.480 --> 00:43:33.160
 mostly those KPIs are passed on Vision Visits.

482
00:43:33.160 --> 00:43:41.600
 Isense, Epsom, and Bose.

483
00:43:44.600 --> 00:43:45.560
 Display compliance. both. Both. Both. Both. Both. Okay.

484
00:43:45.560 --> 00:43:47.520
 Display compliance.

485
00:43:47.520 --> 00:43:50.200
 If you can log in in navigator,

486
00:43:50.200 --> 00:43:54.280
 is that nice to have you like a user?

487
00:43:54.280 --> 00:43:56.680
 You will not have access, but it is.

488
00:43:56.680 --> 00:43:58.960
 Yeah, I'm in right now.

489
00:43:58.960 --> 00:44:02.400
 I just have, it just says no programs available.

490
00:44:02.400 --> 00:44:03.580
 Yeah, yeah.

491
00:44:03.580 --> 00:44:04.420
 Exactly.

492
00:44:04.420 --> 00:44:06.400
 Give me a second to give you access. Yeah, yeah, exactly. Give me a second.

493
00:44:06.400 --> 00:46:13.840
 No problem. Okay. Um. Okay. Sorry. I have my wallet. I'm connecting. OK, Robin is. Also, that does about heavier.

494
00:46:14.560 --> 00:46:25.760
 Yeah, if Robin will be the. That analyst, because why we we we.

495
00:46:28.940 --> 00:46:29.600
 We can provide with you. I mean it's try to profile.

496
00:46:31.000 --> 00:46:31.840
 And he can see everything.

497
00:46:33.100 --> 00:46:34.240
 Yeah, yeah, I think so.

498
00:46:35.520 --> 00:46:37.440
 I want I want to.

499
00:46:39.520 --> 00:46:40.880
 I can set up as a super user.

500
00:46:44.480 --> 00:46:45.140
 Yeah, yeah, but he need to see everything exactly. Yeah, yeah. He need to see everything. Exactly.

501
00:46:45.140 --> 00:46:48.460
 Yeah, let me give him access to everything.

502
00:46:49.380 --> 00:46:50.480
 That's also the user.

503
00:46:52.220 --> 00:47:46.940
 Okay, I'll... Thank you. This is the data export of Epson. This is the sales,

504
00:47:46.940 --> 00:47:54.720
 this product, model, the stores,

505
00:47:54.720 --> 00:47:56.740
 which made the sale,

506
00:47:56.740 --> 00:47:59.820
 the date, and the quantity.

507
00:47:59.820 --> 00:48:02.260
 Is this just a direct representation of

508
00:48:02.260 --> 00:48:05.040
 the slug from Query source or is this?

509
00:48:05.040 --> 00:48:11.040
 Yeah. There are an external loop for getting the raw data of this.

510
00:48:11.040 --> 00:48:16.200
 Got it. This is just a download source for that slug?

511
00:48:16.800 --> 00:48:31.320
 You can hear you waiting on Excel file for this but there are a look for filling this table.

512
00:48:31.320 --> 00:48:36.040
 You can use the loop instead of

513
00:48:36.040 --> 00:48:49.800
 loading the Excel file you can use the loop as well. through query source API.

514
00:48:52.840 --> 00:48:56.840
 OK, can you refresh your browser?

515
00:48:59.160 --> 00:49:01.160
 Yeah, I still see no programs available.

516
00:49:04.680 --> 00:49:05.640
 I can try log out and log back in maybe. Yeah.

517
00:49:05.640 --> 00:49:09.560
 Oh, yeah, there we go. Now I have everything.

518
00:49:09.760 --> 00:49:11.600
 Perfect.

519
00:49:11.600 --> 00:49:14.080
 Okay. Well, guys,

520
00:49:14.080 --> 00:49:17.440
 I think that's really everything I wanted to cover today.

521
00:49:17.440 --> 00:49:20.800
 My main next steps are getting the VPN

522
00:49:20.800 --> 00:49:23.720
 and the environment set up on my machine.

523
00:49:23.720 --> 00:49:27.640
 So I'll probably set up some time on Monday to do that with you have your if

524
00:49:27.640 --> 00:49:30.600
 that's OK. And I'm sure I'll have

525
00:49:30.600 --> 00:49:32.720
 more questions at that point and

526
00:49:32.720 --> 00:49:34.680
 we'll take it from there.

527
00:49:34.680 --> 00:49:36.560
 Perfect perfect.

528
00:49:36.560 --> 00:49:38.280
 OK, well thank you guys so much for

529
00:49:38.280 --> 00:49:40.760
 your time. I really appreciate it.

530
00:49:40.760 --> 00:49:43.520
 Right anytime anytime talk to you later.

531
00:49:43.520 --> 00:49:45.000
 Bye guys bye Bye. Bye.

532
00:49:45.000 --> 00:49:45.880
 Bye bye.
