from typing import Any, Dict, List, Optional, Union, Callable
import uuid
from contextlib import asynccontextmanager
import numpy as np
import sqlalchemy
from sqlalchemy import (
    text,
    Column,
    insert,
    Table,
    MetaData,
    select,
    asc,
    func,
    event,
    JSON,
    Index
)
from sqlalchemy.sql import literal_column
from sqlalchemy import bindparam
from sqlalchemy.orm import aliased
from sqlalchemy.ext.asyncio import (
    create_async_engine,
    AsyncSession,
    AsyncEngine
)
from sqlalchemy.sql.expression import cast
from sqlalchemy.dialects.postgresql import JSONB, ARRAY
from sqlalchemy.orm import (
    declarative_base,
    sessionmaker,
    DeclarativeBase,
    Mapped,
    mapped_column
)
# PgVector
from pgvector.sqlalchemy import Vector
from pgvector.psycopg import register_vector_async
# Datamodel
from datamodel.parsers.json import json_encoder  # pylint: disable=E0611
from navconfig.logging import logging
from .abstract import AbstractStore
from ..conf import default_sqlalchemy_pg
from .models import SearchResult, Document, DistanceStrategy


Base = declarative_base()

def vector_distance(embedding_column, vector, op):
    return text(f"{embedding_column} {op} :query_embedding").label("distance")


class PgVectorStore(AbstractStore):
    """
    A PostgreSQL vector store implementation using pgvector, completely independent of Langchain.
    This store interacts directly with a specified schema and table for robust data isolation.
    """
    def __init__(
        self,
        table: str = None,
        schema: str = 'public',
        id_column: str = 'id',
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        text_column: str = 'text',
        embedding_model: Union[dict, str] = "sentence-transformers/all-mpnet-base-v2",
        embedding: Optional[Callable] = None,
        distance_strategy: DistanceStrategy = DistanceStrategy.COSINE,
        use_uuid: bool = False,
        **kwargs
    ):
        """ Initializes the PgVectorStore with the specified parameters.
        """
        self.table_name = table
        self.schema = schema
        self._id_column: str = id_column
        self._embedding_column: str = embedding_column
        self._document_column: str = document_column
        self._text_column: str = text_column
        self.distance_strategy = distance_strategy
        self._use_uuid: bool = use_uuid
        self._embedding_store_cache: Dict[str, Any] = {}
        super().__init__(
            embedding_model=embedding_model,
            embedding=embedding,
            **kwargs
        )
        self.dsn = kwargs.get('dsn', default_sqlalchemy_pg)
        self._connection: Optional[AsyncEngine] = None
        self.logger = logging.getLogger("PgVectorStore")
        self.embedding_store = None
        if table:
            # create a table definition:
            self.embedding_store = self._define_collection_store(
                table=table,
                schema=schema,
                dimension=self.dimension,
                id_column=id_column,
                embedding_column=embedding_column,
                document_column=self._document_column,
                text_column=text_column,
        )


    def get_id_column(self, use_uuid: bool) -> sqlalchemy.Column:
        """
        Return the ID column definition based on whether to use UUID or not.
        If use_uuid is True, the ID column will be a PostgreSQL UUID type with
        server-side generation using uuid_generate_v4().
        If use_uuid is False, the ID column will be a String type with a default
        value generated by Python's uuid.uuid4() function.
        """
        if use_uuid:
            # DB will auto-generate UUID; SQLAlchemy should not set a default!
            return sqlalchemy.Column(
                sqlalchemy.dialects.postgresql.UUID(as_uuid=True),
                primary_key=True,
                index=True,
                unique=True,
                server_default=sqlalchemy.text('uuid_generate_v4()')
            )
        else:
            # Python generates UUID (as string)
            return sqlalchemy.Column(
                sqlalchemy.String,
                primary_key=True,
                index=True,
                unique=True,
                default=lambda: str(uuid.uuid4())
            )

    def _define_collection_store(
        self,
        table: str,
        schema: str,
        dimension: int = 384,
        metadata: Optional[MetaData] = None,
        id_column: str = 'id',
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        metadata_column: str = 'cmetadata',
        text_column: str = 'text',
        store_name: str = 'EmbeddingStore'
    ) -> Any:
        """Dynamically define a SQLAlchemy Table for pgvector storage.

        Args:
            table: The name of the table to create.
            schema: The schema in which to create the table.
            dimension: The dimensionality of the vector embeddings.
        """
        metadata = metadata or Base.metadata

        fq_table_name = f"{schema}.{table}"
        if fq_table_name in self._embedding_store_cache:
            return self._embedding_store_cache[fq_table_name]

        attrs = {
            '__tablename__': table,
            '__table_args__': {
                "schema": schema,
                "extend_existing": True
            },
            id_column: self.get_id_column(use_uuid=self._use_uuid),
            embedding_column: Column(Vector(dimension)),
            text_column: Column(sqlalchemy.String, nullable=True),
            document_column: Column(sqlalchemy.String, nullable=True),
            metadata_column: Column(JSONB, nullable=True),
            'collection_id': Column(
                sqlalchemy.dialects.postgresql.UUID(as_uuid=True),
                index=True,
                unique=True,
                default=uuid.uuid4,
                server_default=sqlalchemy.text('uuid_generate_v4()')
            )
        }
        # Create dynamic ORM class
        EmbeddingStore = type(store_name, (Base,), attrs)
        EmbeddingStore.__name__ = store_name
        return EmbeddingStore

    def define_collection_table(
        self,
        table: str,
        schema: str,
        dimension: int = 384,
        metadata: Optional[MetaData] = None,
        use_uuid: bool = False,
        id_column: str = 'id',
        embedding_column: str = 'embedding'
    ) -> sqlalchemy.Table:
        """Dynamically define a SQLAlchemy Table for pgvector storage."""
        metadata = metadata or Base.metadata

        columns = []

        if use_uuid:
            columns.append(Column(
                id_column,
                sqlalchemy.dialects.postgresql.UUID(as_uuid=True),
                primary_key=True,
                server_default=sqlalchemy.text("uuid_generate_v4()")
            ))
        else:
            columns.append(Column(
                id_column,
                sqlalchemy.String,
                primary_key=True,
                default=lambda: str(uuid.uuid4())
            ))

        columns.extend([
            Column(embedding_column, Vector(dimension)),
            Column('text', sqlalchemy.String, nullable=True),
            Column('document', sqlalchemy.String, nullable=True),
            Column('cmetadata', JSONB, nullable=True)
        ])

        return Table(
            table,
            metadata,
            *columns,
            schema=schema
        )

    async def connection(self, dsn: str = None) -> AsyncEngine:
        """Establishes and returns an async database connection."""
        if not dsn:
            dsn = self.dsn or default_sqlalchemy_pg
        if self._connection is None:
            try:
                self._connection = create_async_engine(
                    dsn,
                    future=True,
                    pool_size=50,           # High concurrency support
                    max_overflow=100,       # Burst capacity
                    pool_pre_ping=True,     # Connection health checks
                    pool_recycle=3600,      # Prevent stale connections
                    connect_args={
                        "server_settings": {
                            "jit": "off",                    # Disable JIT for vector queries
                            "random_page_cost": "1.1",       # SSD optimization
                            "effective_cache_size": "24GB",  # Memory configuration
                            # "shared_buffers": "8GB",
                            "work_mem": "256MB"
                        }
                    }
                )
                @event.listens_for(self._connection.sync_engine, "connect")
                def register_vector(dbapi_connection, connection_record):
                    register_vector_async(dbapi_connection)

                self.session = sessionmaker(
                    self.engine,
                    class_=AsyncSession,
                    expire_on_commit=False
                )
                self._connected = True
                self.logger.info(
                    "Successfully connected to PostgreSQL."
                )
            except Exception as e:
                self.logger.error(
                    f"Failed to connect to PostgreSQL: {e}"
                )
                self._connected = False
                raise
        return self._connection

    async def initialize_database(self):
        """Initialize with PgVector 0.8.0+ optimizations"""
        async with self.session() as session:
            await session.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))

            # Enable iterative scanning (breakthrough feature)
            await session.execute(text("SET hnsw.iterative_scan = 'relaxed_order'"))
            await session.execute(text("SET hnsw.max_scan_tuples = 20000"))
            await session.execute(text("SET hnsw.ef_search = 200"))
            await session.execute(text("SET ivfflat.iterative_scan = 'on'"))
            await session.execute(text("SET ivfflat.max_probes = 100"))

            # Performance tuning
            await session.execute(text("SET maintenance_work_mem = '2GB'"))
            await session.execute(text("SET max_parallel_maintenance_workers = 8"))
            await session.execute(text("SET enable_seqscan = off"))
            await session.commit()

    async def disconnect(self) -> None:
        """Disposes of the database connection."""
        if self._connection:
            await self._connection.dispose()
            self._connection = None
            self._connected = False
            self.logger.info("PostgreSQL connection closed.")

    async def add_documents(
        self,
        documents: List[Document],
        table: str,
        schema: str = 'public',
        embedding_column: str = 'embedding',
        content_column: str = 'document',
        metadata_column: str = 'cmetadata',
        **kwargs
    ) -> None:
        """
        Embeds and adds documents to the specified table.

        Args:
            documents: A list of Document objects to add.
            table: The name of the table.
            schema: The database schema where the table resides.
            embedding_column: The name of the column to store embeddings.
            content_column: The name of the column to store the main text content.
            metadata_column: The name of the JSONB column for metadata.
        """
        if not self._connected:
            await self.connection()

        texts = [doc.page_content for doc in documents]
        embeddings = self._embed_.embed_documents(texts)
        metadatas = [doc.metadata for doc in documents]

        # Step 1: Ensure the ORM table is initialized
        if self.embedding_store is None:
            self.embedding_store = self._define_collection_store(
                table=table,
                schema=schema,
                dimension=self.dimension,
                id_column=self._id_column,
                embedding_column=embedding_column,
                document_column=content_column,
                metadata_column=metadata_column,
                text_column=self._text_column,
            )

        # Step 2: Prepare values for bulk insert
        values = [
            {
                self._id_column: str(uuid.uuid4()),
                embedding_column: embeddings[i].tolist() if isinstance(
                    embeddings[i], np.ndarray
                ) else embeddings[i],
                content_column: texts[i],
                metadata_column: metadatas[i] or {}
            }
            for i in range(len(documents))
        ]

        # Step 3: Build insert statement using SQLAlchemy's insert()
        insert_stmt = insert(self.embedding_store)

        # Step 4: Execute using async executemany
        try:
            async with self._connection.begin() as conn:
                await conn.execute(insert_stmt, values)
            self.logger.info(
                f"Successfully added {len(documents)} documents to '{schema}.{table}'."
            )
        except Exception as e:
            self.logger.error(f"Error adding documents: {e}")
            raise

    def get_distance_strategy(self, embedding_column_obj, query_embedding, metric: str = None) -> Any:
        """
        Return the appropriate distance expression based on the metric or configured strategy.

        Args:
            embedding_column_obj: The SQLAlchemy column object for embeddings
            query_embedding: The query embedding vector
            metric: Optional metric string ('COSINE', 'L2', 'IP', 'DOT') - if None, uses self.distance_strategy
        """
        # Use provided metric or fall back to instance distance_strategy
        strategy = metric or self.distance_strategy
        self.logger.debug(
            f"PgVector: using distance strategy → {strategy}"
        )

        # Convert string metrics to DistanceStrategy enum if needed
        if isinstance(strategy, str):
            metric_mapping = {
                'COSINE': DistanceStrategy.COSINE,
                'L2': DistanceStrategy.EUCLIDEAN_DISTANCE,
                'EUCLIDEAN': DistanceStrategy.EUCLIDEAN_DISTANCE,
                'IP': DistanceStrategy.MAX_INNER_PRODUCT,
                'DOT': DistanceStrategy.DOT_PRODUCT,
                'DOT_PRODUCT': DistanceStrategy.DOT_PRODUCT,
                'MAX_INNER_PRODUCT': DistanceStrategy.MAX_INNER_PRODUCT
            }
            strategy = metric_mapping.get(strategy.upper(), DistanceStrategy.COSINE)

        self.logger.debug(
            f"PgVector: using distance strategy → {strategy}"
        )

        # Convert numpy array to list if needed
        if isinstance(query_embedding, np.ndarray):
            query_embedding = query_embedding.tolist()

        if strategy == DistanceStrategy.EUCLIDEAN_DISTANCE:
            return embedding_column_obj.l2_distance(query_embedding)
        elif strategy == DistanceStrategy.COSINE:
            return embedding_column_obj.cosine_distance(query_embedding)
        elif strategy == DistanceStrategy.MAX_INNER_PRODUCT:
            return embedding_column_obj.max_inner_product(query_embedding)
        elif strategy == DistanceStrategy.DOT_PRODUCT:
            # Note: pgvector doesn't have dot_product, using max_inner_product
            return embedding_column_obj.max_inner_product(query_embedding)
        else:
            raise ValueError(
                f"Got unexpected value for distance: {strategy}. "
                f"Should be one of {', '.join([ds.value for ds in DistanceStrategy])}."
            )

    async def similarity_search(
        self,
        query: str,
        table: str = None,
        schema: str = None,
        limit: int = None,
        metadata_filters: Optional[Dict[str, Any]] = None,
        score_threshold: Optional[float] = None,
        metric: str = None,
        embedding_column: str = 'embedding',
        content_column: str = 'document',
        metadata_column: str = 'cmetadata',
        id_column: str = 'id',
        additional_columns: Optional[List[str]] = None
    ) -> List[SearchResult]:
        """
        Perform similarity search with optional threshold filtering.

        Args:
            query: The search query text
            table: Table name (optional, uses default if not provided)
            schema: Schema name (optional, uses default if not provided)
            limit: Maximum number of results to return
            threshold: Maximum distance threshold (results with distance > threshold will be filtered out)
            metadata_filters: Dictionary of metadata filters to apply
            metric: Distance metric to use ('COSINE', 'L2', 'IP')
            embedding_column: Name of the embedding column
            content_column: Name of the content column
            metadata_column: Name of the metadata column
            id_column: Name of the ID column

        Returns:
            List of SearchResult objects with content, metadata, score, collection_id, and record_id
        """
        if not self._connected:
            await self.connection()

        table = table or self.table_name
        schema = schema or self.schema

        # Step 1: Ensure the ORM class exists
        if not self.embedding_store:
            self.embedding_store = self._define_collection_store(
                table=table,
                schema=schema,
                dimension=self.dimension,
                id_column=self._id_column,
                embedding_column=embedding_column,
                document_column=content_column,
                metadata_column=metadata_column,
                text_column=self._text_column,
            )

        # Step 2: Embed the query
        query_embedding = self._embed_.embed_query(query)

        # Get the actual column objects
        content_col = getattr(self.embedding_store, content_column)
        metadata_col = getattr(self.embedding_store, metadata_column)
        embedding_col = getattr(self.embedding_store, embedding_column)
        id_col = getattr(self.embedding_store, id_column)
        collection_id_col = getattr(self.embedding_store, 'collection_id')

        # Get the distance expression using the appropriate method
        distance_expr = self.get_distance_strategy(
            embedding_col,
            query_embedding,
            metric=metric
        ).label("distance")
        self.logger.debug(f"Compiled distance expr → {distance_expr}")


        # Build the select columns list
        select_columns = [
            id_col,
            content_col,
            metadata_col,
            distance_expr,
            collection_id_col,
        ]

        # Add additional columns dynamically using literal_column (no validation)
        if additional_columns:
            for col_name in additional_columns:
                # Use literal_column to reference any column name without ORM validation
                additional_col = literal_column(f'"{col_name}"').label(col_name)
                select_columns.append(additional_col)
                self.logger.debug(f"Added dynamic column: {col_name}")

        # Step 5: Construct statement
        stmt = (
            select(*select_columns)
            .select_from(self.embedding_store)  # Explicitly specify the table
            .order_by(asc(distance_expr))
        )

        # Apply threshold filter if provided
        if score_threshold is not None:
            stmt = stmt.where(distance_expr <= score_threshold)

        if limit:
            stmt = stmt.limit(limit)

        # 6) Apply any JSONB metadata filters
        if metadata_filters:
            for key, val in metadata_filters.items():
                stmt = stmt.where(
                    metadata_col[key].astext == str(val)
                )

        try:
            # Execute query
            async with self._connection.connect() as conn:
                result = await conn.execute(stmt)
                rows = result.fetchall()

                print(f'ROW > {rows}')

                # Create enhanced SearchResult objects
                results = []
                for row in rows:
                    metadata = row[2]
                    metadata['collection_id'] = row[4]
                    # Add additional columns as a dictionary (starting from index 5)
                    if additional_columns:
                        for i, col_name in enumerate(additional_columns):
                            metadata[col_name] = row[5 + i]
                    # Create an enhanced SearchResult with additional fields
                    search_result = SearchResult(
                        id=row[0],
                        content=row[1],        # content_col
                        metadata=metadata,       # metadata_col
                        score=row[3]           # distance
                    )
                    results.append(search_result)

                return results
        except Exception as e:
            self.logger.error(f"Error during similarity search: {e}")
            raise

    def get_vector(self, metric_type: str = None, **kwargs):
        raise NotImplementedError("This method is part of the old implementation.")

    async def similarity_search_with_score(self, *args, **kwargs):
        raise NotImplementedError("Use similarity_search instead.")

    async def from_documents(self, *args, **kwargs):
        raise NotImplementedError("Use add_documents instead.")

    async def create_collection(self, *args, **kwargs):
        raise NotImplementedError("Collections are managed as schema.table.")

    async def delete_documents(self, *args, **kwargs):
        raise NotImplementedError("Not implemented in this refactor.")

    async def delete_documents_by_filter(self, *args, **kwargs):
        raise NotImplementedError("Not implemented in this refactor.")


    async def drop_collection(self, table: str, schema: str = 'public') -> None:
        """
        Drops the specified table in the given schema.

        Args:
            table: The name of the table to drop.
            schema: The database schema where the table resides.
        """
        if not self._connected:
            await self.connection()

        full_table_name = f"{schema}.{table}"
        async with self._connection.begin() as conn:
            await conn.execute(text(f"DROP TABLE IF EXISTS {full_table_name}"))
        self.logger.info(f"Table '{full_table_name}' dropped successfully.")


    async def prepare_embedding_table(
        self,
        table: str,
        schema: str = 'public',
        conn: AsyncEngine = None,
        id_column: str = 'id',
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        metadata_column: str = 'cmetadata',
        dimension: int = 768,
        use_jsonb: bool = True,
        drop_columns: bool = False,
        create_all_indexes: bool = True,
        **kwargs
    ):
        """
        Prepare a Postgres Table as an embedding table in PostgreSQL with advanced features.
        This method prepares a table with the following columns:
        - id: unique identifier (String)
        - embedding: the vector column (Vector(dimension) or JSONB)
        - document: text column containing the document
        - collection_id: UUID column for collection identification.
        - metadata: JSONB column for metadata
        - Additional columns based on the provided `columns` list
        - Enhanced indexing strategies for efficient querying
        - Support for multiple distance strategies (COSINE, L2, IP, etc.)
        Args:
        - tablename (str): Name of the table to create.
        - embedding_column (str): Name of the column for storing embeddings.
        - document_column (str): Name of the column for storing document text.
        - metadata_column (str): Name of the column for storing metadata.
        - dimension (int): Dimension of the embedding vector.
        - id_column (str): Name of the column for storing unique identifiers.
        - use_jsonb (bool): Whether to use JSONB for metadata storage.
        - drop_columns (bool): Whether to drop existing columns.
        - create_all_indexes (bool): Whether to create all distance strategies.
    """
        tablename = f"{schema}.{table}"
        # Drop existing columns if requested
        if drop_columns:
            for column in (document_column, embedding_column, metadata_column):
                await conn.execute(
                    sqlalchemy.text(
                        f'ALTER TABLE {tablename} DROP COLUMN IF EXISTS {column};'
                    )
                )
        # Create metadata column as a jsonb field
        if use_jsonb:
            await conn.execute(
                sqlalchemy.text(
                    f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {metadata_column} JSONB;'
                )
            )
        # Use pgvector type
        await conn.execute(
            sqlalchemy.text(
                f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {embedding_column} vector({dimension});'
            )
        )
        # Create additional columns
        for col_name, col_type in [
            (document_column, 'TEXT'),
            (id_column, 'varchar'),
        ]:
            await conn.execute(
                sqlalchemy.text(
                    f'ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS {col_name} {col_type};'
                )
            )
        # Create the Collection Column:
        await conn.execute(
            sqlalchemy.text(
                f"ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS collection_id UUID;"
            )
        )
        await conn.execute(
            sqlalchemy.text(
                f"ALTER TABLE {tablename} ADD COLUMN IF NOT EXISTS collection_id UUID DEFAULT uuid_generate_v4();"
            )
        )
        # Set the value on null values before declaring not null:
        await conn.execute(
            sqlalchemy.text(
                f"UPDATE {tablename} SET collection_id = uuid_generate_v4() WHERE collection_id IS NULL;"
            )
        )
        await conn.execute(
            sqlalchemy.text(
                f"ALTER TABLE {tablename} ALTER COLUMN collection_id SET NOT NULL;"
            )
        )
        await conn.execute(
            sqlalchemy.text(
                f"CREATE UNIQUE INDEX IF NOT EXISTS idx_{table}_{schema}_collection_id ON {tablename} (collection_id);"
            )
        )
        # ✅ CREATE COMPREHENSIVE INDEXES
        if create_all_indexes:
            print("🔧 Creating indexes for all distance strategies...")
            # COSINE index (most common for text embeddings)
            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_cosine "
                    f"ON {tablename} USING ivfflat ({embedding_column} vector_cosine_ops);"
                )
            )
            print("✅ Created COSINE index")
            # L2/Euclidean index
            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_l2 "
                    f"ON {tablename} USING ivfflat ({embedding_column} vector_l2_ops);"
                )
            )
            print("✅ Created L2 index")
            # Inner Product index
            try:
                await conn.execute(
                    sqlalchemy.text(
                        f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_ip "
                        f"ON {tablename} USING ivfflat ({embedding_column} vector_ip_ops);"
                    )
                )
                print("✅ Created Inner Product index")
            except Exception as e:
                print(
                  f"⚠️ Inner Product index creation failed: {e}"
                )
            # HNSW indexes for better performance (requires more memory)
            try:
                await conn.execute(
                    sqlalchemy.text(
                        f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_hnsw_cosine "
                        f"ON {tablename} USING hnsw ({embedding_column} vector_cosine_ops);"
                    )
                )
                print("✅ Created HNSW COSINE index")
                await conn.execute(
                    sqlalchemy.text(
                        f"""CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_hnsw_l2
                            ON {tablename} USING hnsw ({embedding_column} vector_l2_ops) WITH (
                            m = 16,  -- graph connectivity (higher → better recall, more memory)
                            ef_construction = 200 -- controls indexing time vs. recall
                        );"""
                    )
                )
                print("✅ Created HNSW EUCLIDEAN index")
            except Exception as e:
                print(f"⚠️ HNSW index creation failed (this is optional): {e}")

        else:
            # Create index only for current strategy
            distance_strategy_ops = {
                DistanceStrategy.COSINE: "vector_cosine_ops",
                DistanceStrategy.EUCLIDEAN_DISTANCE: "vector_l2_ops",
                DistanceStrategy.MAX_INNER_PRODUCT: "vector_ip_ops",
                DistanceStrategy.DOT_PRODUCT: "vector_ip_ops"
            }

            ops = distance_strategy_ops.get(self.distance_strategy, "vector_cosine_ops")
            strategy_name = str(self.distance_strategy).rsplit('.', maxsplit=1)[-1].lower()

            await conn.execute(
                sqlalchemy.text(
                    f"CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_{strategy_name} "
                    f"ON {tablename} USING ivfflat ({embedding_column} {ops});"
                )
            )
            print(f"✅ Created {strategy_name.upper()} index")

        # Create JSONB indexes for better performance
        await self._create_jsonb_indexes(
            conn,
            tablename,
            metadata_column,
            id_column
        )
        # Ensure the table is ready for embedding operations
        self.embedding_store = self._define_collection_store(
            table=table,
            schema=schema,
            dimension=dimension,
            id_column=id_column,
            embedding_column=embedding_column,
            document_column=self._document_column
          )
        return True

    async def create_embedding_table(
        self,
        table: str,
        columns: List[str],
        schema: str = 'public',
        embedding_column: str = 'embedding',
        document_column: str = 'document',
        metadata_column: str = 'cmetadata',
        dimension: int = None,
        id_column: str = 'id',
        use_jsonb: bool = True,
        drop_columns: bool = True,
        create_all_indexes: bool = True,
        **kwargs
    ):
        """
        Create an embedding table in PostgreSQL with advanced features.
        This method creates a table with the following columns:
        - id: unique identifier (String)
        - embedding: the vector column (Vector(dimension) or JSONB)
        - document: text column containing the document
        - cmetadata: JSONB column for metadata
        - Additional columns based on the provided `columns` list
        - Enhanced indexing strategies for efficient querying
        - Support for multiple distance strategies (COSINE, L2, IP, etc.)
        Args:
        - table (str): Name of the table to create.
        - columns (List[str]): List of column names to include in the table.
        - schema (str): Database schema where the table will be created.
        - embedding_column (str): Name of the column for storing embeddings.
        - document_column (str): Name of the column for storing document text.
        - metadata_column (str): Name of the column for storing metadata.
        - dimension (int): Dimension of the embedding vector.
        - id_column (str): Name of the column for storing unique identifiers.
        - use_jsonb (bool): Whether to use JSONB for metadata storage.
        - drop_columns (bool): Whether to drop existing columns.
        - create_all_indexes (bool): Whether to create all distance strategies.

        Enhanced embedding table creation with JSONB strategy for better semantic search.

        This approach creates multiple document representations:
        1. Primary search content (emphasizing store ID)
        2. Location-based content
        3. Structured metadata for filtering
        4. Multiple embedding variations
        """
        tablename = f'{schema}.{table}'
        cols = ', '.join(columns)
        _qry = f'SELECT {cols} FROM {tablename};'
        dimension = dimension or self.dimension

        # Generate a sample embedding to determine its dimension
        sample_vector = self._embed_.embedding.embed_query("sample text")
        vector_dim = len(sample_vector)
        self.logger.notice(
            f"USING EMBED {self._embed_} with dimension {vector_dim}"
        )

        if vector_dim != dimension:
            raise ValueError(
                f"Expected embedding dimension {dimension}, but got {vector_dim}"
            )

        async with self._connection.begin() as conn:
            result = await conn.execute(sqlalchemy.text(_qry))
            rows = result.fetchall()

            await self.prepare_embedding_table(
                tablename=tablename,
                embedding_column=embedding_column,
                document_column=document_column,
                metadata_column=metadata_column,
                dimension=dimension,
                id_column=id_column,
                use_jsonb=use_jsonb,
                drop_columns=drop_columns,
                create_all_indexes=create_all_indexes,
                **kwargs
            )

            # Populate the embedding data
            for i, row in enumerate(rows):
                _id = getattr(row, id_column)
                metadata = {col: getattr(row, col) for col in columns}
                data = await self._create_metadata_structure(metadata, id_column, _id)

                # Generate embedding
                searchable_text = data['structured_search']
                print(f"🔍 Row {i + 1}/{len(rows)} - {_id}")
                print(f"   Text: {searchable_text[:100]}...")

                vector = self._embed_.embedding.embed_query(searchable_text)
                vector_str = "[" + ",".join(str(v) for v in vector) + "]"

                await conn.execute(
                    sqlalchemy.text(f"""
                        UPDATE {tablename}
                        SET {embedding_column} = :embeddings,
                            {document_column} = :document,
                            {metadata_column} = :metadata
                        WHERE {id_column} = :id
                    """),
                    {
                        "embeddings": vector_str,
                        "document": searchable_text,
                        "metadata": json_encoder(data),
                        "id": _id
                    }
                )

        print("✅ Updated Table embeddings with comprehensive indexes.")

    def _create_natural_searchable_text(
        self,
        metadata: dict,
        id_column: str,
        record_id: str
    ) -> str:
        """
        Create well-structured, natural language text with proper separation.

        This creates clean, readable text that embedding models can understand better.
        """
        # Start with the ID in multiple formats for exact matching
        text_parts = [
            f"ID: {record_id}",
            f"Identifier: {record_id}",
            id_column + ": " + record_id
        ]

        # Process each field to create natural language descriptions
        for key, value in metadata.items():
            if value is None or value == '':
                continue
            clean_value = value.strip() if isinstance(value, str) else str(value)
            text_parts.append(f"{key}: {clean_value}")
            # Add the field in natural language format
            clean_key = key.replace('_', ' ').title()
            text_parts.append(f"{clean_key}={clean_value}")

        # Join with spaces and clean up
        searchable_text = ', '.join(text_parts) + '.'

        return searchable_text

    def _create_structured_search_text(self, metadata: dict, id_column: str, record_id: str) -> str:
        """
        Create a more structured but still readable search text.

        This emphasizes key-value relationships while staying readable.
        """
        # ID section with emphasis
        kv_sections = [
            f"ID: {record_id}",
            f"Identifier: {record_id}",
            id_column + ": " + record_id
        ]

        # Key-value sections with clean separation
        for key, value in metadata.items():
            if value is None or value == '':
                continue

            # Clean key-value representation
            clean_key = key.replace('_', ' ').title()
            kv_sections.append(f"{clean_key}: {value}")
            kv_sections.append(f"{key}: {value}")

        # Combine with proper separation
        return ' | '.join(kv_sections)

    async def _create_metadata_structure(
        self,
        metadata: dict,
        id_column: str,
        _id: str
    ):
        """Create a structured metadata representation for the document."""
        # Create a structured metadata representation
        enhanced_metadata = {
            "id": _id,
            id_column: _id,
            "_variants": [
                _id,
                _id.lower(),
                _id.upper()
            ]
        }
        for key, value in metadata.items():
            enhanced_metadata[key] = value
            # Create searchable variants for key fields
            if value and isinstance(value, str):
                variants = [value, value.lower(), value.upper()]
                # Add variants without special characters
                clean_value = ''.join(c for c in str(value) if c.isalnum() or c.isspace())
                if clean_value != value:
                    variants.append(clean_value)
                enhanced_metadata[f"_{key}_variants"] = list(set(variants))
        # create a full-text search field of searchable content
        enhanced_metadata['searchable_content'] = self._create_natural_searchable_text(
            metadata, id_column, _id
        )

        # Also create a structured search text that emphasizes important fields
        enhanced_metadata['structured_search'] = self._create_structured_search_text(
            metadata, id_column, _id
        )

        return enhanced_metadata

    async def _create_jsonb_indexes(
        self,
        conn,
        tablename: str,
        metadata_col: str,
        id_column: str
    ):
        """Create optimized JSONB indexes for better search performance."""

        print("🔧 Creating JSONB indexes on Metadata for optimized search...")

        # Index for ID searches
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_{id_column}
                ON {tablename} USING BTREE (({metadata_col}->>'{id_column}'));
            """)
        )
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_id
                ON {tablename} USING BTREE (({metadata_col}->>'id'));
            """)
        )

        # GIN index for full-text search on searchable content
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_fulltext
                ON {tablename} USING GIN (to_tsvector('english', {metadata_col}->>'searchable_content'));
            """)
        )

        # GIN index for JSONB structure searches
        await conn.execute(
            sqlalchemy.text(f"""
                CREATE INDEX IF NOT EXISTS idx_{tablename.replace('.', '_')}_metadata_gin
                ON {tablename} USING GIN ({metadata_col});
            """)
        )
        print("✅ Created optimized JSONB indexes")
